{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [04] 모델 만들기2 - AuxConv\n",
    "\n",
    "`AuxConv`는 `BaseConv`에서 추출된 mid(18x18), end(9x9) featuremap 중에서 end featuremap을 받아와, 여러 스케일의 featuremap으로 학습하는 모듈입니다. 우리는 mid, end와 그에 대해 AuxConv를 수행한 4개의 추가적인 feature map들(9x9, 5x5, 3x3, 1x1)을 활용하여 `PredConv`에서 객체 검출(Object Detection)을 수행할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "`AuxConv` 는 여러 층의 Conv Layer에 featuremap을 통과시키면서 다양한 scale의 featuremap을 얻습니다. 아래는 VGG-Net을 기반으로 만든 SSD 검출기의 auxiliary conv층을 표현하고 있습니다. 아래의 이미지에서는 19x19의 feature map을 사용하고 있습니다.\n",
    "\n",
    "- [참고] SSD(Single Shot Multibox Detector) 논문 : https://arxiv.org/pdf/1512.02325.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='img/[04]auxconv.png'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# Image from https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
    "display(HTML(\"<img src='img/[04]auxconv.png'>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## [Task 1] `AuxConv`로 featuremap 얻기\n",
    "\n",
    "`AuxConv`에서는 18x18의 feature map을 받아 conv layer에 통과시킴으로써 9x9, 5x5, 3x3, 1x1의 feature map들을 얻습니다. \n",
    "\n",
    "### ToDo : AuxConv 연습하기\n",
    "\n",
    "먼저, 아래의 `auxconv_practice`함수를 완성하여 5x5의 feature map을 얻어봅시다. 필요한 구성은 다음과 같습니다.\n",
    "\n",
    "- `conv1` : 1x1 convolution (out_channels=256, stride=1, padding=0)\n",
    "- activation : relu\n",
    "- `conv2` : 3x3 convolution (out_channels=512, stride=2, padding=1)\n",
    "- activation : relu\n",
    "\n",
    "9x9의 feature map을 위의 layer들에 통과시킴으로써, 5x5의 feature map을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 feature map의 shape: torch.Size([4, 512, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def auxconv_practice(featuremap):\n",
    "    \"\"\"featuremap을 받아 auxiliary conv를 통과시킵니다.\"\"\"\n",
    "    \n",
    "    # [ToDo]: conv1을 구성합니다.\n",
    "    conv1 = nn.??????(1280, ????, kernel_size=???, stride=???, padding=???)\n",
    "    \n",
    "    # [ToDo]: conv2를 구성합니다.\n",
    "    conv2 = nn.Conv2d(????, ????, kernel_size=???, stride=???, padding=???)\n",
    "    act = nn.ReLU()\n",
    "    \n",
    "    output = act(conv1(featuremap))\n",
    "    output = act(conv2(output))\n",
    "    \n",
    "    return output\n",
    "\n",
    "sample = torch.randn(4, 1280, 18, 18)\n",
    "output = auxconv_practice(sample)\n",
    "\n",
    "print('결과 feature map의 shape: {}'.format(output.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## [Task 2] AuxConvluition Class\n",
    "\n",
    "`AuxConv`도 이전 실습의 `BaseConv`와 마찬가지로 `nn.Module` Class로 만들어보도록 하겠습니다. 각 결과 feature map은 1x1 conv와 3x3 conv를 각각 한 번씩 거칠 때마다 얻게 됩니다.\n",
    "\n",
    "### ToDo: nn.Module Class로 통합하기\n",
    "\n",
    "`__init__()` 함수를 완성합니다. 각 layer의 구성은 다음과 같습니다.\n",
    "- conva_1 : 1x1, (out_channels=256, stride=1, padding=0)\n",
    "- conva_2 : 3x3, (out_channels=512, stride=2, padding=1)\n",
    "- convb_1 : 1x1, (out_channels=128, stride=1, padding=0)\n",
    "- convb_2 : 3x3, (out_channels=256, stride=2, padding=1)\n",
    "- convc_1 : 1x1, (out_channels=128, stride=1, padding=0)\n",
    "- convc_2 : 3x3, (out_channels=256, stride=1, padding=0)\n",
    "- convd_1 : 1x1, (out_channels=128, stride=1, padding=0)\n",
    "- convd_2 : 3x3, (out_channels=256, stride=1, padding=0)\n",
    "\n",
    "`forward` 함수를 완성합니다.\n",
    "- 2개의 conv층을 거칠 때마다 feature map 하나를 얻습니다.\n",
    "- 각 conv층의 뒤에 relu activation 연산을 합니다.\n",
    "- 9x9, 5x5, 3x3, 1x1의 feature map을 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AuxConvolution(nn.Module):\n",
    "    \n",
    "    # [ToDo]: __init__ 함수를 완성합니다. use_bias로 bias 사용 여부를 선택 가능하도록 합니다.\n",
    "    def __init__(self, ????????):\n",
    "        super(AuxConvolution, self).__init__()\n",
    "        self.conv_a1 = nn.Conv2d(1280, 256, kernel_size=1, stride=1, padding=0, bias=??????)\n",
    "        self.conv_a2 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=??????)\n",
    "        \n",
    "        self.conv_b1 = nn.Conv2d(512, 128, kernel_size=1, stride=1, padding=0, bias=??????)\n",
    "        self.conv_b2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=??????)\n",
    "        \n",
    "        self.conv_c1 = nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0, bias=??????)\n",
    "        self.conv_c2 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0, bias=??????)\n",
    "        \n",
    "        self.conv_d1 = nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0, bias=??????)\n",
    "        self.conv_d2 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0, bias=??????)\n",
    "        \n",
    "        self.init__conv2d(??????)\n",
    "        \n",
    "        \n",
    "    def init__conv2d(self, ??????):\n",
    "        \"\"\"weight와 bias를 initialize합니다.\"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                if ??????:\n",
    "                    nn.init.constant_(c.bias, 0.)\n",
    "        \n",
    "    \n",
    "    # [ToDo]: forward 함수를 완성합니다.\n",
    "    def forward(self, end_features):\n",
    "        \"\"\"\n",
    "        end featuremap을 받아 conv layers를 통과시켜 9x9, 5x5, 3x3, 1x1의 feature map을 얻습니다.\n",
    "        \"\"\"\n",
    "        out = ???????????????????\n",
    "        out = ???????????????????\n",
    "        features_a = out\n",
    "        \n",
    "        out = ???????????????????\n",
    "        out = ???????????????????\n",
    "        features_b = out\n",
    "        \n",
    "        out = ???????????????????\n",
    "        out = ???????????????????\n",
    "        features_c = out\n",
    "        \n",
    "        out = ???????????????????\n",
    "        out = ???????????????????\n",
    "        features_d = out\n",
    "        \n",
    "        return features_a, features_b, features_c, features_d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class를 완성했다면, BaseConvolution과 연동하여 AuxConvolution을 거친 featuremap의 결과값을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "\n",
      "mid shape torch.Size([4, 112, 18, 18])\n",
      "end shape torch.Size([4, 1280, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "from materials.det_modules import BaseConvolution\n",
    "\n",
    "baseconv = BaseConvolution()\n",
    "\n",
    "sample = torch.randn(4, 3, 300, 300)\n",
    "mid, end = baseconv(sample)\n",
    "\n",
    "print('\\nmid shape {}'.format(mid.shape))\n",
    "print('end shape {}'.format(end.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "map a shape : torch.Size([4, 512, 9, 9])\n",
      "map b shape : torch.Size([4, 256, 5, 5])\n",
      "map c shape : torch.Size([4, 256, 3, 3])\n",
      "map d shape : torch.Size([4, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# AuxConvolution을 테스트합니다.\n",
    "\n",
    "# [ToDo]: AuxConvolution을 생성합니다.\n",
    "auxconv = ???????????????????\n",
    "\n",
    "# [ToDo]: end feature를 통과시켜 4개의 featuremap을 얻습니다.\n",
    "??????????????????? = ???????????????????\n",
    "\n",
    "print('\\nmap a shape : {}'.format(features_a.shape))\n",
    "print('map b shape : {}'.format(features_b.shape))\n",
    "print('map c shape : {}'.format(features_c.shape))\n",
    "print('map d shape : {}'.format(features_d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### <생각해 봅시다>\n",
    "\n",
    "- 1x1 conv와 3x3 conv가 하는 역할은 무엇인가요?\n",
    "- 왜 다양한 크기의 feature map을 구성하고자 하나요?\n",
    "\n",
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
