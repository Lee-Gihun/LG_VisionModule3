{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [09] 모델 학습하기\n",
    "\n",
    "데이터, 라벨, 모델, 손실함수가 모두 준비되었기에 모델을 학습할 수 있게 되었습니다. 모델을 학습하기 위해서 우리든 다음의 요소들을 정의합니다.\n",
    "\n",
    "- Dataloader\n",
    "- Model\n",
    "- Criterion\n",
    "- Optimizer\n",
    "- Scheduler\n",
    "\n",
    "이외에 학습을 위해서 다양한 Hyperparameter들을 설정해야 합니다. 아래의 그림을 통해 Batch와 Epoch, Iteration에 대한 개념을 정리하고 설정된 학습 요소들을 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# Image from https://www.slideshare.net/w0ong/ss-82372826\n",
    "display(HTML(\"<img src='img/[09]epoch.png'>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as scheduler\n",
    "\n",
    "from materials.DetectionNet import DetectionNet, create_prior_boxes\n",
    "from materials.MultiboxLoss import MultiBoxLoss\n",
    "from materials.datasets import PascalVOCDataset\n",
    "from materials.utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = PascalVOCDataset(data_folder='./data/VOC', split='TRAIN')\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=8, shuffle=True, \n",
    "                              collate_fn=dataset.collate_fn, num_workers=4)\n",
    "\n",
    "model = DetectionNet(n_classes=20, unfreeze_keys=['15', 'head', 'bn1'], use_bias=True).to(device)\n",
    "criterion = MultiBoxLoss(priors_cxcy=create_prior_boxes(), threshold=0.5, neg_pos_ratio=0.3, alpha=1.0)\n",
    "\n",
    "num_epochs = 50\n",
    "lr, momentum, weight_decay = 1e-3, 0.9, 5e-4\n",
    "\n",
    "biases, not_biases = [], []\n",
    "\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param_name.endswith('.bias'):\n",
    "            biases.append(param)\n",
    "        else:\n",
    "            not_biases.append(param)\n",
    "optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                            lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=False)\n",
    "scheduler = scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.00005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## [Task 1] Training Loop 만들기\n",
    "Training 과정에서 Batch마다, Epoch마다 어떠한 동작을 수행하는지를 지정함으로써 학습이 진행되도록 만들어 봅시다.\n",
    "\n",
    "### ToDo: `train` 함수 완성하기\n",
    "\n",
    "train 함수를 완성하고 학습을 시작해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler=None, \n",
    "          num_epochs=200, grad_clip=None, print_freq=1, \n",
    "          save_name='test', device=device):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "        start = time.time()\n",
    "    \n",
    "        for i, (images, boxes, labels, _) in enumerate(dataloader):\n",
    "            data_time.update(time.time()-start)\n",
    "\n",
    "            images = images.to(device)\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "\n",
    "            # [ToDo]: forward pass를 만듭니다.\n",
    "            pred_locs, pred_scores = ??????????\n",
    "            \n",
    "            # [ToDo]: loss를 계산합니다.\n",
    "            loss = ??????????????????????????????????\n",
    "            \n",
    "            if loss > 100:\n",
    "                continue\n",
    "                \n",
    "            # [ToDo]: optimizer의 gradient를 초기화힙니다.\n",
    "            ????????????????????\n",
    "            \n",
    "            # [ToDo]: loss에 대한 gradient를 계산합니다.\n",
    "            ????????????????????\n",
    "            \n",
    "            if grad_clip is not None:\n",
    "                clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "            # [ToDo]: optimizer를 통해 parameter를 업데이트합니다.\n",
    "            ?????????????????????\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(dataloader),\n",
    "                                                                      batch_time=batch_time,\n",
    "                                                                      data_time=data_time, loss=losses))\n",
    "        if scheduler is not None:\n",
    "            # [ToDo]: secheduler를 동작시켜 learning rate을 업데이트합니다.\n",
    "            ??????????????????\n",
    "            \n",
    "            \n",
    "    torch.save(model.state_dict(), './{}_{}.pth'.format(save_name, round(losses.val, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# train을 수행합니다.\n",
    "train(model, dataloader, criterion, optimizer, scheduler, \n",
    "      num_epochs=5, grad_clip=None, print_freq=50, save_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### <생각해 봅시다>\n",
    "\n",
    "- Learing Rate을 달리해가며 초기 학습(< 4epoch)을 살펴봅시다. 어떤 경향을 가지고 있나요?\n",
    "- Gradient를 Clip한다는 것은 어떤 의미를 가질까요?\n",
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
